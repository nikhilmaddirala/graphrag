Nikhil Maddirala (00:10)
Welcome to the Art and Science of AI, a podcast about the science of how AI works and the art of using AI to change the world. I'm Nikhil Matirala, an AI product manager.

Piyush (00:22)
I'm P .U. Sugarwal. I'm an AI sales executive. I'm very, very curious about these things. Nikhil, the last time, like since we spoke an hour and year back, I've been reflecting on just like the hype around AI, right? Like I'm sure all of us are aware of all the hype that's going on. And I usually try and avoid people that hype up things. But what I believe what's different this time is that the kind of the...

The people that are hyping this are people that I deeply admire and respect. So, for example, like one of the examples of a hype around AI is, let's take the example of Sundar Pichai. I really respect and admire Sundar Pichai. I feel like he's a very measured person and he chooses his word very carefully. And he said this now in many interviews. So it's not just like he said it once, but many interviews. And he said that AI will have a more profound impact on humanity than fire or electricity.

which I feel is the biggest hype that I've ever heard about AI. And I've been so like, is that like, is that hype real? And the reason why I think like it's the biggest hype is I mean, I have some understanding of the impact that fire had on humanity. There's a anthropologist called Richard Wrangham. He wrote a very interesting book. I'm not sure if you know this called Catching Fire. And one of the core idea that he shares in that book is like fire literally shaped humanity.

And the way it works is before we used to control fire, we were spending like hours, like chewing food and digesting food, like our ape ancestors. And once we started controlling fire and cooking food with fire, think of like cooking food as like pre -digesting food. And that saved up a lot of the time, but it also like saved up a lot of the energy, in like digesting that food, right? Like, and all of that energy savings that were going to our intestines basically went into our head and our brain sizes started becoming bigger.

And that led to like this whole chain of event, like bigger brain sizes. We started being born prematurely because it was hard to like birth humans, right, with bigger heads and which is why we're so malleable. I say all that to say is like fire like literally shaped humans. So if Sundar Pichai is saying that artificial intelligence will have a bigger impact on humanity than fire did, I don't think you can hype up AI more than that, right? So I'm just very curious, like is all that hype real? And the other thing,

is like you're also one of those people that I deeply admire and respect because I what I've noticed about you knowing you over the last two decades is you don't easily buy into hype. So you're very level headed and measured about like being very logical and like looking at evidence. And I feel like I notice that you also bought into the hype a little bit, not as big as fire or electricity. But I remember you brought this idea to me, which isn't your original idea. I think you attributed to Jensen Huang, who's the CEO of Nvidia, that

AI is having its iPhone moment. This is what you told me, which I found really insightful last year. And like iPhone is, I mean, I think that iPhone is the greatest piece of technology that humanities ever created. It's an amazing, amazing gadget, right? So I think it's a big bold claim that the AI can have as big of an impact as the iPhone did, right? So I'm curious to like hear your point of view on like this big hype around AI and like the...

like larger traction, but I know these things can be hard to discuss. So maybe a more pointed question that I want to ask you is last year, you told me this idea that AI is having an iPhone moment. Do you believe that that happened? Like is AI having its iPhone moment? Like what's your review after one year?

Nikhil Maddirala (03:55)
Yeah, that's a great question. I think starting with the long -term impact is an interesting thing to think about. It reminds me of this quote I've often heard. I don't know who said it first, but that often with new technology, we tend to overestimate the impact that we'll have in one year, and we tend to underestimate the impact that it will have in like a 10 -year horizon. And in this case, it seems like...

Piyush (04:19)
Interesting.

Nikhil Maddirala (04:23)
We, I definitely bought into that hype of like overestimating the impact it will have in one year. And I think many people did, but in terms of the long -term impact, it's, it's not clear what it'll be. Like, I do believe that it will be a very transformative, technology shift and whether it's on the scale of fire. I'm not sure, but I think the iPhone moment is a good comparison because I think it's likely going to be.

something similar to that where we might see a new paradigm of the way in which people engage with technology. And last time in the intro, we talked about how software is eating the world and now that's transforming to AI is eating the world. So I definitely think that the long -term trend is heading that way, but there was a lot of hype, as I said, like people often overestimate based on this long -term impact, like how much we can actually do in one year. And I think...

Last year, there was a ton of hype around how AI will immediately transform so many businesses and applications. There's been a ton of investment in the area. And now a lot of people are questioning whether that hype was valid or not. And I think in this episode, yeah, go ahead.

Piyush (05:37)
Interesting. Like, if I'm hearing you correctly, are you saying that the AI didn't end up having its iPhone moment, or at least in the last year? Like, that's very surprising to me because all I've heard is AI AI.

Nikhil Maddirala (05:47)
Yeah, I mean, it certainly didn't have its iPhone moment yet because I think having an iPhone moment. Well, I mean, it's hard to when did iPhone have its iPhone moment? That's actually an interesting question because the iPhone came out in 2007. But when it first came out, it wasn't the transformative technology that we now understand it to be when the iPhone first came out. There were no third party apps.

Piyush (06:08)
Hmm.

Nikhil Maddirala (06:10)
you could use the iPhone only to use apps that Apple themselves had provided. There was no concept. They didn't even know that they were going to get into a third party app store. And really, I think the iPhone took off when third party apps took off and people were able to build innovative new businesses like Uber, which is fundamentally impossible in a free iPhone world. So things like Uber, ride sharing, delivery, so many things that were just not possible. So,

Piyush (06:32)
Right.

Nikhil Maddirala (06:38)
It's not a single moment, perhaps five years later, we might look back at this and say, Hey, this kind of was the iPhone moment, but it's, I think, just taking a lot longer than we initially thought it would. And I wanted to open with some interesting observations I've seen from people in the field, starting with this quote from professor Galloway, who's a marketing professor at.

Piyush (07:02)
Mmm, yeah I know. He's great.

Nikhil Maddirala (07:05)
Yeah, I think he's at NYU or Columbia or one of these places, but he was tweeting about...

Piyush (07:09)
I've heard some of his heartaches. He has great heartaches and just popular culture in general, I feel like.

Nikhil Maddirala (07:16)
Yeah, I mean, so I don't necessarily agree with this, but I just want to point this out. So he's talking about us being in an AI bubble and he cites this chart from The Economist.

Piyush (07:26)
So it just, I mean, a lot of people will be listening to this. I just want to make sure we can explain what we're looking at. So we're looking at this chart, which shows market value added versus expected revenue. And what are we looking at, Nikhil? Like, what is this? What are those two?

Nikhil Maddirala (07:39)
Yeah. So his claim on this thread is that we're in an AI bubble and he cites this chart from The Economist and it shows two figures. The first one is showing how much market value was added due to AI. And the second figure is how much revenue is expected from AI in 2024. So the first figure shows that $3 trillion of market value was added due to AI. And this is primarily in the

Piyush (07:58)
wow.

Nikhil Maddirala (08:06)
increased valuations of companies like Nvidia, Amazon, Microsoft, Google. If you look at the total increase in the market cap, that's gone up by three trillion. And if you actually look at the expected revenue from AI this year in 2024, that's only about 20 billion. So there's a big gap between these two. I'm not sure if this is an accurate comparison. Of course, there are many caveats because market value really

Piyush (08:09)
Right.

Yeah.

Nikhil Maddirala (08:35)
is the stock market's

Piyush (08:35)
It's hard to attribute it just to AI, right? Like there's so many. Yeah, I hear you. But I think I see the point you're making though.

Nikhil Maddirala (08:40)
True. And it's based on long -term valuation. When market value goes up, that's because investors believe that in the long term, companies are going to generate this revenue. But I think there's also a lot of expectations around short -term, how much value we'd get. And so far, companies have been struggling to deliver a lot of value in the short term.

Piyush (08:46)
Right.

Right.

Yeah. Yeah.

By the way, great way to plug in threads. You're the only person I know who uses threads. So go meta. Love it.

Nikhil Maddirala (09:06)
I've been active a lot on threads. Yeah, and actually so as we go through this podcast I think you'll see that a lot of the Resources I bring are going to be from there because they're just from resources I consume so the next Chart I want to show is This one it's from Gartner. This was actually published towards the end of last year It's on the generative AI hype cycle. So there's a general concept of

Piyush (09:18)
Cool.

Hmm.

Nikhil Maddirala (09:34)
called the hype cycle that various technologies go through. And there are these different phases. So first, there's like an innovation trigger where the hype is like expectations are climbing, climbing really high. And then we kind of reach a peak of inflated expectations, which I think potentially could be happening at this point or sometime in the recent past. And then it goes through.

Piyush (09:37)
Interesting.

Nikhil Maddirala (10:01)
like a trough of disillusionment where people are like, no, like none of this stuff was actually going to be realized. But then over time it comes back up and there's a slope of enlightenment and a plateau of productivity. Yeah, actually the play. Yeah, I think this is very true. So you could say for the dot com bubble, maybe sometime around 1999, 2000, we were at like the peak of inflated expectations and.

Piyush (10:12)
Yeah, this is very cool. I guess you could use this framework to like analyze the dot com bubble and the internet, right? Like a...

Right.

Nikhil Maddirala (10:30)
stock prices and the entire market fell sharply but then later on like if we look at it now twenty plus years later we're probably even way beyond the expectations the the hype expectations that were there in the beginning.

Piyush (10:43)
Are you telling me I should short the NASDAQ top 10? Nikhil Madhurala is not giving investment advice at this point.

Nikhil Maddirala (10:48)
No, I am not telling you you should do that. Because yeah, like the stock market again is based on long term value and we're talking about like short term impact. So anyway, and another quote that I wanted to share on this topic was from another person I follow on threads, Carnage for Life. He's actually a colleague of mine of Metta.

Piyush (10:58)
Right, right.

This is very interesting.

Nikhil Maddirala (11:15)
And this thread says, I sometimes wonder if LLMs are this generation's self -driving cars. A decade ago, self -driving cars were so promising, it was assumed that they were 90 % of the way to being ubiquitous. But now it looks like the last 10 % of the work is taking 90 % of the time. LLMs work well 90 % of the time. The question is, how hard will it be to address that last 10 %? And I think it's really this last 10 % that

Piyush (11:32)
Hmm.

Interesting.

Nikhil Maddirala (11:45)
people have been really struggling with. It's really easy to build a quick demo and throw it up on Twitter or YouTube and show your application working in a couple of use cases. But the moment you try to scale that to have like meaningful business impact, you start realizing that LLMs have a lot of edge cases and points of failure. And it's really hard to build a reliable like end to end application

Piyush (12:13)
we were all super mesmerized and taken away by like the 90 % that the LLMs were able to do when they launched and it was so great, right? But the reason why we haven't like reached this iPhone moment, which is hard to kind of define, but it hasn't kind of had the impact that we're hoping it would is because that last 10 % has been so hard to solve.

Nikhil Maddirala (12:32)
Yeah, I think that's exactly the case. And that's the same thing we've seen in self -driving cars. I remember, like this thread says about 10 years ago, everyone kept saying self -driving cars are just a couple of years away. They're a couple of years away. And in all streets in the US, there are going to be self -driving cars. But that's proved like really difficult. And it's those edge cases they're really hard to solve for. And what's interesting is that LLMs have very specific

types of failure and that's actually what I think would be interesting to do.

Piyush (13:05)
Yeah, I was going to ask you that. So I understand what you're saying. So like with self -driving cars, things like stopping at the right place or picking up someone where it's kind of like a weird intersection, those edge cases are really hard. So I am curious, like what are these edge cases in the context of LLMs?

Nikhil Maddirala (13:22)
For sure. So before we get into that, I think there are like a couple of different things going on why building LLM based applications is hard. One is the thing we just talked about, like self -driving, the last 10 % problem is really hard to solve and figuring out how to get those edge cases. That's one thing. A second reason, which I think we won't get to in this episode, but we should talk about in a future episode is platform and ecosystem challenges. This is a significant...

bottleneck for LLM applications to really be successful because right now we're still in the old iPhone era and our access to the digital world is controlled by Apple and Google who control the operating systems for iOS and Android and there are certain things that they will let applications do and certain things that they prevent applications from doing. And so for example, I would love to be able to...

Piyush (14:13)
Hmm.

Nikhil Maddirala (14:17)
replace Siri with chat GPT or some other assistant on my iPhone but that's fundamentally not possible and not because of failure and maybe some of it is but there's even if they solve that

Piyush (14:21)
Right.

It's just that that application won't have access to like camera or maybe some things that are exclusive to Siri, right? Like if I'm understanding you correctly, right.

Nikhil Maddirala (14:33)
Yeah, like voice assist, you can't trigger it by voice, you can't use it hands -free. So there are a lot of strategic ecosystem and company level challenges here that are... And actually one example of this, I think, where a product failed for multiple reasons, but including this platform and ecosystem failure is these new AI assistants that we've seen. So there's... Sorry, can we... One sec.

These, so you might have heard of these products called the Humane AI Pen.

Piyush (15:03)
yeah, yeah, yeah. Apparently Marcus Brownlee by himself took down Hewmane. He had like a review for the AI pin for Hewmane. And this is so consequential. I don't know if this is true or not, but a lot of people are saying that Marcus Brownlee alone like basically destroyed this company with this review. I don't know how.

Nikhil Maddirala (15:13)
It's right here. It says the worst product I've ever reviewed.

Well, he actually made a follow -up video which you can see on the bottom of the screen that says do bad reviews kill companies and he said no bad products kill companies not bad reviews. So I'm actually with MKBHD on this one, but that's a separate topic. So this...

Piyush (15:30)
okay. Interesting. Right.

But that's a very interesting perspective. Sorry, before you move on, I never thought about it like this. You're saying that these companies like Humane and Rabbit, they have to create their own devices because of the platform problem. Like they're not able to do what they would have. Like for example, Rabbit, I think they were trying to do like large action model, right? Like they want to take certain actions and like Apple just wouldn't allow them to take certain actions if they were to build. Interesting. Wow.

Nikhil Maddirala (15:53)
Yes.

Yeah, absolutely. So there is no reason for these products to exist as a separate device outside of the device that you already carry everywhere with you, your smartphone. But that device is locked down and controlled by Apple and Google. And so these products failed for a number of other reasons. Let's just be clear. Like they suffered from this last 10 % problem actually getting all the edge cases and doing useful things.

Piyush (16:10)
Yeah.

Nikhil Maddirala (16:30)
They failed because they didn't do that well, and we'll talk about that. But even if they succeeded at that, another reason why they failed is because of problems with platforms and ecosystems. And that's...

Piyush (16:41)
That's so interesting. So in a funny way, like the reason why AI is not able to have its iPhone moment is literally because of iPhone and some.

Nikhil Maddirala (16:49)
Yeah, that's actually a really interesting observation. And I think that's why people are trying to build a new iPhone. So for example, the company I work for Meta, we're investing heavily in virtual reality, augmented reality and wearable devices. So we have a smart glasses that you can use that has AI built into that. And a big part of the reason... Yeah, it's really cool. But a big part of the reason why is...

Piyush (17:04)
Right.

You have one, right? At some point you should wear it in one of the episodes.

Right?

Nikhil Maddirala (17:18)
wants to invest in this space is because he wants the next generation of ecosystem to not be controlled by Apple's closed model. And he talks about how he wants to promote like a more open model. But we can talk about that in a future episode. So yeah, just pointing out that there is another reason why these things have been failing. But.

Piyush (17:35)
It's a very insightful perspective. Yeah.

Actually, can I ask you one thing I'm thinking and we don't have to discuss in this episode, but at some point it sounds like you're already concluding that the future of AI will be LLMs. Like LLM is just one type of AI, right? Like, and you keep referring to LLMs when you're talking about like hype and all of this. So are you also trying to say that LLMs are the future?

Nikhil Maddirala (18:05)
That's actually the third point that I was going to bring up, which is we don't even know if LLMs are the future. And now LLMs are expanding into multimodal foundation models, but let's just call them multimodal LLMs for now. Like we don't have a good name for them. So far it is the most general purpose AI architecture that we have found that works better than anything else we've done. And I think there's a debate on both sides of this. A lot of companies, I think like,

Piyush (18:11)
Right.

Nikhil Maddirala (18:34)
opening i believe that scaling this same architecture further and further with more data more compute will eventually get us to solve these problems but for example there's famously some people who reject that idea young lacoon is a famous example he's one of the godfathers of a i and he's currently the chief a i a scientist at meta and he is fundamentally against the idea that just scaling l l m's will

Piyush (18:52)
Right.

Nikhil Maddirala (19:02)
solve the intelligence problem. And he has a different architecture. Yeah, he has a different architecture that he proposes about how models should build what he calls a world model. It's a bit complicated to get into, but the key distinction is that what LLMs are doing is they take input and they convert it into an abstract representation in

Piyush (19:05)
Does he offer an alternative?

Nikhil Maddirala (19:27)
like an abstract representation space, like there are vector embeddings or it's like a latent space where it's representing the input, but then it's directly producing output. That's either text or image or video. And his idea is many things that there should be a feedback layer between it should first predict the representation layer and then use that to make conclusions about the actual output layer.

And there are many components to his idea of how this should work and there's a research program at Metta that's trying to make that work. So that's also another unknown, whether LLMs themselves are good enough.

Piyush (19:56)
Right.

So all this is a debate on the architecture. So LLM being one architecture for enabling AI, but the debate is like what the right architecture. Okay, I understand. So there are three things you talked about. You talked about some at the last 10 % being very difficult. You talked about the platform issues and you talked about the architecture. I'm very curious about the edge cases part. Like I don't fully understand like what are those edge cases that we're not able to.

Nikhil Maddirala (20:11)
Yes. Yeah.

Yeah.

Yeah. So.

Yeah. And I think that's useful to focus on because the last two things are kind of outside of our control. So if you were thinking about building an LLM application or, you know, using AI applications to solve your own problems right now, that's the main thing you can focus on. Like how can I use existing technology, understanding its limitations and trying to solve for that. Whereas the other two.

Piyush (20:54)
because the scope of the other two is just too huge for like a small startup to handle.

Nikhil Maddirala (20:57)
Yeah, exactly. The only people who can solve that ecosystem problem are like large players and same with like foundational AI research is there are like a few companies that are investing in this and they're already thinking about how to solve this. So the main problems with LLMs, what caused them to result in these failure points are a few things. Like one is that LLMs are frozen in time.

Piyush (21:07)
Makes sense.

interesting.

Nikhil Maddirala (21:25)
and they lack up -to -date knowledge. So the way it works is you get a huge amount of training data and you train the LLM and that's the internal knowledge that it has. And this training process, as we talked about, is hugely expensive and time -consuming. For these large models, the cost is in the order of tens of millions of dollars and I think several months of time to do this. So they're not refreshed.

Piyush (21:30)
Mmm.

Yeah, I remember there was a there was a time where ChaiGBD would often respond with as of my training update for last, I don't know, October of 2020, something like that. So, OK, that makes sense.

Nikhil Maddirala (22:01)
Yeah, and they've been trying to get better at that and we'll talk about how they're doing it. So that's the first thing they're frozen in

Piyush (22:07)
But what is the problem there? Are you saying the problem is it doesn't have access to the most recent real time information, that that's the failure point? OK.

Nikhil Maddirala (22:16)
Yeah, exactly. So if I build an AI application and the user wants to ask something about something that happened recently, that's fundamentally not possible. And in response to that, what LLMs do is they tend to hallucinate often. So hallucination is the problem where when you ask an LLM question about something domain specific,

or something up to date that it doesn't have knowledge of, it tends to like hallucinate rather than admitting ignorance. And hallucinating is like it will generate a plausible sounding answer. So based on its training data, it's been trained to give responses to questions that make the questioner like happy or like produce a positive reaction. And it's essentially lying.

Piyush (23:04)
lying what you're choosing to use the word hallucinate because there is no person with motivations or whatever but it's essentially lying to you

Nikhil Maddirala (23:12)
Well, yeah, lying implies motivation also, but it doesn't know the concept of truth and lie and any of that stuff. So it's just trained. It's a pattern recognition engine. It's been trained with huge amounts of text data and it's trying to reproduce like plausible sounding text that is related to what you asked about.

Piyush (23:18)
Right.

I see why that's a problem, but that's so cool though, right? Like even hallucination by itself is like such an amazing emergent property of these LLMs. The fact, like why isn't this machine just saying, I don't know, or like, why is it making up stuff? That's such a cool thing.

Nikhil Maddirala (23:45)
That's just how it was trained. So I think people are trying to fix that problem. And you have to differentiate two different kinds of use cases. One where hallucination is actually a good thing, and that's often in creative endeavors. So if I'm asking the LLM to help me do something creative, like give me ideas for a podcast, give me ideas for an article I want to write, like help me write. So these are all creative pursuits, and there is no right or wrong answer there. So.

Piyush (23:59)
Hmm.

Nikhil Maddirala (24:12)
whatever the LLM comes up with is often good input for you. So in those areas it works well, but if you want to use the LLM for more factual use cases, then this is horrible for you because if you're like, hey, I want, tell me what email I got, tell me what's the status of my flight, like any of these kinds of things. Yeah, absolutely. So anything where you want factual information, this is a huge problem. So the problem is they...

Piyush (24:32)
There's something medical related, right?

Nikhil Maddirala (24:40)
They're frozen in time, there's no up to date knowledge and they lack domain specific knowledge. So they're trained on like just general purpose text. They don't know the answers to questions about maybe your own company's internal documents, something about your life, something about your specific context or your business's context. It doesn't know any of that. So, and yeah, we talked about the reason why this happens is because.

Piyush (24:42)
Right.

Right.

Nikhil Maddirala (25:05)
machine learning models and LLMs in general over several decades have been trained to generalize rather than memorize. So the goal is to take the input that it's given, the training data and generate output that is similar to what the training data looks like, but not exactly the training data. And so because of that, this is just a natural property that

Piyush (25:29)
Hmm.

Interesting. What was it you said again? Generalize not what? That's interesting. Okay.

Nikhil Maddirala (25:34)
LLMs have, they tend to hallucinate. Memorize. They're trained to not memorize because actually when you want, like suppose you, in classical machine learning systems, I gave it, say it was something to predict like a house price or something based on characteristics. I give it certain data points. My goal is not for it to remember those data points.

The goal is for it to understand the underlying pattern of those data points. And then when I give it a new input, it should apply that pattern to that input. But my goal is not for it to have memorized those individual data points because that's not useful for me. So similarly,

Piyush (26:14)
And that's also one cool thing about human intelligence is what we do, right? It's just we like observe these patterns and we try and generalize our learnings from that and we apply it to new situations. Make sense?

Nikhil Maddirala (26:26)
Yep, absolutely. But I think the, we have higher level concepts of understanding when we are grounded in facts versus when we're not. Although even that is a hard skill often.

Piyush (26:40)
Dude, I hallucinate sometimes. Like I've often had situations where I will say something that I believe to be factually correct. And Monica, my wife, will say that, no, no, that's not what happened. And I'm not trying to lie. I just honestly, it came to me as a memory of something factual. I mean, maybe it doesn't happen to me as often as it does to an AI, but I can see it happening to me sometimes. Like, for example, like a lot of times I'll be fully convinced that, yes, I did leave the keys there. Hello. Hi.

Nikhil Maddirala (26:47)
Mm -hmm.

Yeah. Yeah, I think that's a good analogy. Yeah. So imagine that on a larger.

Piyush (27:08)
I will actually get a memory of me leaving the keys somewhere. And then it'll turn out that the keys weren't there. It was somewhere else. So I know for a fact later that I did hallucinate. But at that moment, I believed that to be the truth. So I wonder if something like that's going on with the AI as well.

Nikhil Maddirala (27:25)
Yeah, it's like that on a much larger scale. Whenever it doesn't have the right facts, it just makes up something that sounds plausible. So I can show you some examples of this. Give me a second. So we looked at these reviews. So Neela Patel, reckless1280 on thread, he recently interviewed Sundar Pichai. Actually, this might be the interview you were.

Piyush (27:27)
Right, right.

Interesting. What are some other types of like edge?

there.

He's the editor in chief of Verge, right?

Nikhil Maddirala (27:55)
Of the verge, right. And so he asked Chad GPT -40 to summarize his conversation with Sundar and it returned a full hallucination complete with citations to things we did not talk about at all. That's what he says. So if you click on this thing, he just asks it, can you summarize this interview? And then the Chad GPT -40, which is like the latest and probably the most powerful like model we have, is just making up random stuff. It basically takes this string of text.

Piyush (28:07)
Hehehehehe

Right.

Nikhil Maddirala (28:24)
the URL and then it infers based on that what this conversation might have been about and then it produces a plausible.

Piyush (28:31)
so it doesn't even have the ability to go to that link and actually read through the transcript of the interview. It's just, it's just guessing all of this based on the hyperlink.

Nikhil Maddirala (28:40)
It's just guess exactly the text of the URL is what it's using. It has the word Sundar, Pichai, AI, search, Gemini, future. This is like a SEO optimized link.

Piyush (28:50)
Okay, so the ideal output in this case would have been that, hey, based on the link, there isn't much I can infer. But if you were to give me the transcript of the interview, I can maybe then summarize. okay.

Nikhil Maddirala (28:54)
Sorry I can't access this.

Yes, that exactly would have been the ideal output, but that's not the default way in which LLMs respond and they have to be specifically trained to do that. And he's been like on a crusade, like talking a ton about how LLMs are hallucinating. And he's given all these examples of how like the Google AI overview is generating weird answers about presidents having Corvettes and Trump.

Piyush (29:07)
Right. Interesting. Interesting.

Well, one of the critiques with Google has been that it's been very slow to roll out these AI overviews or search generative experiences, but maybe this is like a, like this is to like Google's point is it's these things are hard to get right. And like a lot of like Google's credibility hinges on the quality of these answers. So yeah, maybe that's why they've been slow to roll it out.

Nikhil Maddirala (29:46)
Yeah, absolutely. So I think this is the point, like going back to the iPhone moment and why it's taking so long is because Google could have put out a product that's like right 90 % of the times. They could have done that a long time ago, but they're really struggling to solve that last 10 % and that's taking a really long time. So hence the slow rollout and same thing as...

Piyush (29:52)
Hmm interesting

Are there other things in the last 10 % like hallucination? I understand that hallucination is a big problem, but is that the only thing that we need to solve or are there other?

Nikhil Maddirala (30:17)
That's the main thing. The other things are just like ways of operationalizing your system and your checks and balances. So having proper evaluation criteria that catches the hallucination and the incorrect responses. So just like operations, having a clean operations pipeline where you have very solid evaluation criteria for when your...

application is producing hallucinated output versus like legitimate output and that itself is yes

Piyush (30:48)
But it sounds like even that is related to the hallucination problem. Are there other fundamentally different types of problems with LLMs right now?

Nikhil Maddirala (30:57)
There are other fundamental problems with LLMs. One of them is that it's often not capable of what we call system two thinking. So there's this concept of system one versus system two thinking, which was introduced by Daniel Kahneman and Amos Tversky, where system one is your immediate response to something. When I ask you a question, what would you immediately say? Whereas system two is when...

Piyush (31:11)
Daniel Kahneman.

Nikhil Maddirala (31:23)
You step back for a moment, you think, you analyze, and then you provide a response to that. So most LLMs are engaging in the system one thinking right now, and getting them to incorporate system two thinking is also a problem. But that has a lot of pretty good solutions actually. In fact, I'd love to talk about that in the next episode, which is like how to build AI based applications for yourself and start using AI so you can.

The ways in which you solve that is you can just have a single LLM and give it instructions to go through a chain of thought process or thinking and then producing a response. You can also have multi -agent systems where you have one LLM producing an output and then there's another LLM that's reviewing that and providing feedback and iterating on it. So there are some decent paradigms to solve that problem. But yeah, these are kind of two of the major.

Piyush (32:12)
Interesting.

Interesting. But I hear your point that the hallucination problem is more fundamental because if you don't solve that, then even this multi -agent system will have that problem. So you need to figure out. So are there solutions to this hallucination problem?

Nikhil Maddirala (32:30)
Yeah, the main solution we have to this right now is a paradigm called retrieval augmented generation. So what that means is that instead of, so the problem is that the LLM doesn't have the information or the knowledge necessary to answer the question that you ask. And the solution to that is to provide all that information and context to the LLM and then ensure that.

the answer it provides is grounded in that specific context that you have provided. So let's take an example. Suppose like I have my own company and I want to ask some questions about my company policies or some internal documentation at my company. So I might ask the LLM, hey, well, what's my company's PTO policy? And just at a base level, it would...

probably give me a hallucinated response or it might say something unhelpful like that I don't know. So that would be a better response to say I don't know, but often it could just give you a hallucinated response also. So the way people are solving that is by connecting the LLMs to an external knowledge base. And then when here actually I can show you a diagram of what this is going to look like that might be helpful.

So here this actually something we talked about even in our last season's podcast This again is by another colleague of mine because and it provides the overview for how retrieval augmented generation works But yeah, this is the paradigm that currently is being leveraged by most companies to use

Piyush (33:58)
Yeah.

Right. Yeah, I've had some strands to reflect on this since you shared it with me, essentially help me understand if I'm thinking about this correctly. What you're trying to do is you're trying to enrich the prompt itself with the most useful context. And all this business of like vector databases and all this is to enrich that prompt, right? It's so that within the prompt itself, the LLM has all the necessary information required to answer that question.

And then you're also limiting the prompt to like make sure that the LLM only answers from this context. So you're trying to limit the hallucination. Am I thinking about?

Nikhil Maddirala (34:47)
Yes, 100%. That is exactly correct. That's exactly what's happening. So if we walk through this chart here, it starts with an original question the user has, which is something about a private document. But the problem is that we don't know which document has the answer to the question that the user has, or if there is any document that has. So maybe I have my company's entire internal documentation. It could be thousands of pages. It could be tens of thousands of pages.

Piyush (35:00)
Right.

Right? Right.

Nikhil Maddirala (35:16)
We don't know. And in many use cases, suppose you are a legal company and you want to read through some legal documents and answer them. They're often just huge, or in the context of academic research. So a naive solution would be, hey, can we just give all this data to the LLM and then ask it this question? But LLMs currently are limited by limited context size.

That means that I can only give a certain amount of text as input to the model and get a certain amount of text as output. That's actually been improving. Yes.

Piyush (35:46)
Right.

And it also costs money, right? No, it's improving, but it also costs money. So like it's a very expensive currency is the prompt size.

Nikhil Maddirala (35:57)
Yes. I mean, the cost has also been improving. If you look at the cost, as new models release, they keep coming down because they're getting more efficient, but yes, you're right. cost is a significant concern. And also even the LLMs these days that have really large token sizes, like Gemini has 2 million tokens. Now, some of Claude's models have large token sizes. They suffer from an information retrieval problem. Still. It's sometimes called the lost in the middle problem.

It's sometimes called the needle in the haystack problem, which is that if you give it just like a large chunk of text, like if you give it an entire book and ask it for information that's somewhere in the middle, it won't know. So the less context you can provide to the LLM, the more effective it will be in using that context to answer your question. So the idea is.

Piyush (36:39)
Right.

Right. Which is where the retrieval part of the RIG comes in, is that retrieval has to be so good that you only retrieve the most relevant types of things to put in your prompt, I guess.

Nikhil Maddirala (36:58)
Yes, exactly. So you have to provide a knowledge base to a vector storage database where you store all of the documents that you have. And there's a lot of interesting strategies you have to consider on how to even store your knowledge base because you have to think about what's the size of each document? How do I logically group the individual documents together? And this area is called chunky, which is how to chunk your knowledge base.

Piyush (37:21)
Right.

Nikhil Maddirala (37:27)
to effectively store it and there are many strategies for that. There are simple strategies from just like, you know, breaking it down by certain number of characters, there's a recursive chunking, there's, these are something called semantic chunking and agentic chunking, which is where you actually use LLMs again to help you figure out like, how do I group this data, these documents together and how do I appropriately chunk them. So there are many problems that have to be solved.

Piyush (37:37)
Right.

Understood. Can I ask one thing? I think I understand how this retrieval augmented generation framework is a solution to the hallucination problem because it's enriching the prompt with the most useful context. My question is, if you can do that effectively, can you ensure, I shouldn't use the word guarantee because it's hard to guarantee things, but can you with reasonable confidence ensure that the LLM will not hallucinate?

if the most pertinent information to answer your question is within the prompt itself.

Nikhil Maddirala (38:23)
Yeah, that's an interesting question. I think you can get to something like from 90 % to 99%, but you're never going to get to like a hundred percent and guarantee that the LLM will like never hallucinate. So in addition to this, we would also implement strategies for reviewing the output of the LLM and checking if that's a hallucination or not. So that that's an advanced rag strategy where you take the answer that the LLM gave.

and you take the provided context and you can actually pass it to another LLM call and you ask the question, is this answer grounded in this context or not? And if not, you don't accept it and you either tell the user that the query failed or you try to do it again. But even with all of this, I think it's so far, it's not a hundred percent. I think you can go from something like 90 % success to 99 % success, but.

Piyush (38:57)
Interesting.

Interesting.

Yeah.

Nikhil Maddirala (39:20)
yeah, I think solving that last 1 % is still really hard and we haven't like figured it out all the way, but this strategy gets you like 99 % of the way there and it's like good enough for most use cases.

Piyush (39:24)
Right.

Yeah, as you're talking about this, I'm thinking you mentioned that interview with Sundar Pichai and then you mentioned like how the AI overviews were hallucinating. Are these AI overviews or like these new types of search engines like perplexity? Are they essentially like a rag on top of the open internet?

Nikhil Maddirala (39:50)
Perplexity is exactly a rag. So I mean, they're probably doing a lot more complex stuff, but the basic foundational architecture is a rag. So perplexity actually calls a Google search API on the backend. In future, they might switch that to some other search API, like they could use Bing or other search providers. But when you run a search, it then runs that search API, gets the search results.

and then uses that as context for answering your query. And it does some retrieval there too, because there's search results. It has to make choices, like how many search results to choose. Maybe it chooses the top 10, but the content of those top 10 may be still too long. So it goes through each of the search results and then chooses which parts are most relevant to you. And then it ties those together, puts it into the context for the prompt and gives you an answer back. So that's exactly how it works.

Piyush (40:29)
Right.

That's very cool. And the perplexity is a great example of like a very successful startup. I mean, they're getting, raising a lot of money, right? Like they're raising a lot of money from very wealthy investors. They've raised billions of dollars. And now that you're mentioning it, I'm thinking like, they're basically using Google's search results for the retrieval part, like all the ranking. They're using some top foundational LLM models to like, as their LLM. So basically they've created like this billion dollar company using retrieval from Google.

foundational models from like other foundational providers. And there's so much opportunity out there for entrepreneurs to like use this rack framework to like create something else, right? This is such an amazing opportunity. And you don't, to your point earlier, you don't need to build like these foundational things.

Nikhil Maddirala (41:23)
Yeah, I mean...

Right. That's absolutely true. I think it is a two -sided coin though. On the one hand, yes, there's an opportunity. You can just tie together existing models, existing data, existing APIs and build an application that solves a problem. But on the other hand, then you don't have much of a competitive advantage or a moat around your company because the people that own the data or the APIs or the surface.

can easily do something like that themselves. Like in this case, for example, Google could easily build a competitor product to perplexity. And there's not much differentiation that perplexity could offer than if all they're doing. So I think one of the...

Piyush (41:57)
Yeah.

It's actually just one thought on what you just said. It's funny you said that. Google IO recently happened and there was a YouTuber who was interviewing Sundar Pichai. And Sundar Pichai actually said something similar to what you're saying. He was giving advice to new startups for trying to build in the world of AI. And he said something to the effect I'm paraphrasing is don't just build a simple wrapper around the foundational models. Try to add some value because if you just build a wrapper, which is something which doesn't have enough mode and just relying completely on the foundational.

Nikhil Maddirala (42:14)
Mm -hmm.

Piyush (42:38)
You'll always live in this existential crisis. It's like, like you're one update away from being like irrelevant, right? From the foundational model. So I think that what you said, that makes a lot of sense.

Nikhil Maddirala (42:44)
Yeah.

Absolutely. This also reminds me of a quote I recently heard from Sam Altman, the OpenAI CEO, who said there are two kinds of startups that you can build using AI today. And he said one is there's a kind of startup that is building something based on today's LLM capabilities and solving problems that exist due to today's LLM capabilities.

And then there are some startups that are building for tomorrow's LLM capabilities and solving problems that will still exist tomorrow. So he was saying, try not to be in the first category where you're building something that's just a problem that is temporary and that will soon go away because the foundation models themselves or someone else will just eat up that capability. So I think that's useful advice to.

Piyush (43:34)
That's great advice. Actually, now that I'm thinking about it, Sundar Pichai did say that in second part of his answer is like, try and think where the industry is going, where the foundational models are going and like try and build. I would love to like learn more about like where these other elements are going, like in our future episodes, because I want to build that faculty in my mind is like, if I'm not doing something, like hopefully one of our listeners ends up, you know, building something amazing. So like, these are great advices, what you're sharing.

Nikhil Maddirala (43:58)
Yeah, I, for sure. I'd love to actually, I think in the next episode, what I'd love to do is just start from the basics for how someone can get started doing things with LLMs. If someone is like, I have no idea what, I've never used AI. I don't know what I can use it for. How do I get started? And we can take them through like the different levels of how they can just start with using basic chatbots and then build more advanced things. I think that would be super cool. One thing I wanted to add on,

Piyush (44:25)
I'd love that.

Nikhil Maddirala (44:27)
the entrepreneurship point that you are making. So yes, one is this, like finding opportunities where you can differentiate yourself when you're building an AI based product or application. And how can you differentiate yourself? There are different ways in which you could do it. One is through the model you're using. Maybe your model is really unique, but that's really hard to do because models are kind of becoming commoditized and all the big companies. Yes, absolutely.

Piyush (44:35)
Right.

And like you said, they're also expensive to make, right? To train.

Nikhil Maddirala (44:56)
So model differentiation is hard unless you're like, you know, a large tech company. The second is differentiation based on unique data that you have or unique user experiences that you're solving for. The data one, I think is a good one. A lot of companies are sitting on useful data that they can use to provide unique AI experiences. So.

Piyush (45:08)
Right. Right.

Nikhil Maddirala (45:23)
I mean, again, most of the data is with big tech companies, so they do have the advantage, but whatever business you're working in, think about what unique data you have access to and how you can leverage that. The second is like actually around differentiating yourself through a good user experience. So instead of just making a general purpose chatbot, if you can build the solution in such a way that it fits into your users workflow.

Piyush (45:35)
Right.

Nikhil Maddirala (45:52)
that often is a real differentiation. So for example, we've been doing podcasting recently, right? And I've been looking at a lot of AI tools that can help you with your podcast, like help you generate show notes, like chapter summaries, timestamps, and things like that. You could obviously use a general purpose chat bot to, to do that. but I think some companies have figured out that there's a good user experience for it. Like maybe if you put it inside the.

podcast editing app that you already use and you just have a button where you click there and says, okay, it automatically generates the show notes for you. And it understands something about the structure of like what podcasters are looking for when they're generating show notes. that is another potential type of differentiation because anyone can build a podcast show notes generator, but the value you're offering is like an integrated experience that fits into a person's overall workflow.

Piyush (46:43)
Interesting.

In our last conversation, you mentioned another point of differentiation. I'm not sure if you're going to mention it again, or these are the only ones you wanted to mention. I'm curious, like why you didn't mention that, which is fine tuning. You were very passionate about that approach to differentiate yourself. We had like this long discussion, great discussion on like, are we eventually, and by the way, I realized that the word I was looking for has been coined by Nick Bostrom and he coined the term Singleton, which is like this ultimate API that can...

Like basically take any input in any modality and give you the desired output, right? Like, so what stops the LLMs? And I think you're making a great, you're advocating on why you shouldn't be like worried about things which are not grounded in reality. Like, which is why I like you. You're not like buying into these hype. You're like, what, let's talk about what's real right now. And then you made a very passionate advocacy for fine tuning. Is that still the case or is that not the case?

Nikhil Maddirala (47:20)
Hmm.

Ahem.

I think, so there are different approaches, three broad approaches you can take to LLMs. One is training your own LLM from scratch. Second is fine tuning an LLM, which is less expensive than training your own LLM, but still more expensive than a RAG. And then there's RAG, Retrieval Augmented Generation. I think for most use cases, the RAG approach is good enough for like 90 % of use cases that people have, you don't need a fine tuned LLM.

Piyush (48:01)
Hmm, interesting.

Nikhil Maddirala (48:06)
the general purpose one is already good enough and you just need to give it the right context and information. But there are certain cases where fine tuning would benefit. So the advantage, the point of fine tuning is not to give it new information because you could provide new information through the RAG approach that we talked about. And also the foundation models themselves are expanding to become multimodal and all of that. Fine tuning is needed when you need a specific style.

if you need the responses to follow a specific stylistic guide, like you're like, I want the responses to be exactly in this way. And if it's something that is hard to describe, but you can show with examples. So one example of fine tuning would be if I want to generate an LLM that sounds exactly like Piusch, then what I can do is get a whole bunch of data of recordings of you, maybe your podcast, other recordings. Yes.

Piyush (48:46)
Interesting.

Interesting.

Or like Scarlett Johansson. Anyways. That's very interesting. Dude, these are like such amazing ideas. Like I want to learn more about each one of them, but I feel like I should ground myself in the way you were describing it is, and maybe we should do that in the next episode is if I'm someone who doesn't know much about like rag, it sounds like a such a complicated abbreviation, retrieval, augmented generation. It's kind of overwhelming. So maybe you can like help me.

Nikhil Maddirala (49:05)
or like Scarlett Johansson. So I...

Yeah, I think that would be a great next.

Piyush (49:30)
Like we can discuss on like, if I'm someone who's very interested in this as you and I are, and want to like figure out like what's the best, like give me some ideas on like how I should think about this. What are some things I should watch out for and like stuff like that. I think we should talk about that.

Nikhil Maddirala (49:43)
For sure. I think that would make a great topic for our next episode. I don't think I have much to add here right now. The only thing I wanted to add on this topic is actually another category of opportunities that we didn't discuss is what's called picks and shovels. And there's a famous saying that when there's a gold rush, the people who get rich are the people who sell shovels. And I think...

Piyush (50:07)
Evidence of that being Nvidia's stock value.

Nikhil Maddirala (50:10)
Nvidia is one for sure, but there are also many other providers of picks and shovels in this place. For example, Pinecone, companies that provide vector databases, companies that provide tooling that enable you to bring your data and models into one place. Cloud computing, for example, I think in general is a picks and shovels enabler in the AI space right now. So yeah, but yes, but that's...

Piyush (50:19)
Right.

You mean the hyperscalers? Interesting. It sounds like this approach, the picks and shovels approach is beyond someone with limited resources, right?

Nikhil Maddirala (50:38)
Yeah, that's not something like small companies can

Yes, that's true. I mean, maybe there are small picks and troubles you could make as someone with limited resources. For example, like there are these libraries like Lang chain and weights and biases. Yeah.

Piyush (50:52)
Hmm.

You have a good, yeah, I was going to ask. Lama index. I think they do a great job in making building rags easier.

Nikhil Maddirala (51:02)
Yeah, sure. So I mean, a lot of these are open source and it's not clear how you can make money for them. But I do think there are small opportunities like this where you're building something not as the end application itself, but just providing some tooling or infrastructure or something that makes it easier for other people to build these kinds of applications.

Piyush (51:16)
Right.

You know what we should do is like in our future episode, we should kind of like do a double click or a deep dive in each one of these opportunities. Just like, I feel like today we like talked about all of these at the, at a high level and it was a great overview of the landscape and where we're at right now. But yeah, I would love to learn more about each one of these things.

Nikhil Maddirala (51:27)
Yeah.

Maybe let's start one episode with just a very basic how -to guide, taking someone from who's like, I've never used LLMs or generative AI before. How do I get started? And we can talk about what are the things you can do with basic chatbots? Where do you need some advanced features? And then when do you need to build custom applications? And then we could do a separate episode on, OK, now that you understand all the different things you can do.

What are the opportunities that you could explore here? Does that sound good?

Piyush (52:07)
Yeah, that'd be great. And I think that's where you come in. That's where your expertise come in. Because for most people like me, there are unknown unknowns. I don't even know what I don't know. So how do I even structure my approach? And I think with your expertise, you can help structure our discussions in a way that lets someone get a better understanding step by step.

Nikhil Maddirala (52:26)
Alright, alright Piyush, well this has been a great conversation as always and we'll see you in the next episode.

Piyush (52:33)
Yeah, it was great. Thank you. See you in the next episode. This is great.

