00:00:00:00 - 00:00:00:22
Nikhil Maddirala
one example

00:00:00:22 - 00:00:03:06
Nikhil Maddirala
for neural networks is,

00:00:03:06 - 00:00:04:13
Nikhil Maddirala
image recognition.

00:00:04:13 - 00:00:08:17
Nikhil Maddirala
it's a bunch of images of handwritten digits from 0 to 9.

00:00:08:17 - 00:00:15:19
Nikhil Maddirala
And the goal, the task there is to figure out from the image what number is represented.

00:00:15:19 - 00:00:20:09
Piyush Agarwal
how do you even get the machine to see that. Yeah. Handwritten thing like,

00:00:21:05 - 00:00:23:22
Piyush Agarwal
Because the machine only reads zeros and ones.

00:00:23:22 - 00:00:24:12
Nikhil Maddirala
Yeah.

00:00:24:15 - 00:00:34:26
Piyush Agarwal
First you have to tell the machine that this within the zeros and ones there is like a white paper in which there is a squiggly line in this pattern like that itself is seems so challenging.

00:00:34:27 - 00:00:44:00
Nikhil Maddirala
So that's actually a great question. I think that's a really good segue. You can't do that with this, but you can do it with neural networks. And I can explain to you exactly how we

00:00:44:00 - 00:01:01:24
Speaker 1
Hey, welcome to the art and science of AI journeys. In the season one, which we recorded in May 2023 as a continuous three hour long conversation. And season two is now live. So if you're enjoying this discussion, please subscribe for new episodes every week.

00:01:01:24 - 00:01:03:29
Piyush Agarwal
Okay. So next. Good. Let's go beyond classical.

00:01:03:29 - 00:01:04:00
Nikhil Maddirala
Yeah

00:01:04:00 - 00:01:11:09
Nikhil Maddirala
That's the next thing here. So the concept of a neural network it comes from it's inspired by the human brain,

00:01:11:09 - 00:01:14:10
Nikhil Maddirala
where the brain consists of a bunch of neurons,

00:01:14:10 - 00:01:17:05
Nikhil Maddirala
that are either activated or not. And each neuron.

00:01:17:05 - 00:01:20:03
Nikhil Maddirala
So we have a bunch of neurons that are interconnected. Yeah.

00:01:20:05 - 00:01:26:12
Nikhil Maddirala
And neurons activate in regular patterns, like when you see an image of,

00:01:26:12 - 00:01:39:08
Nikhil Maddirala
or when you have like a happy mode, say, when you see your daughter, right? And you're happy, you have some emotion that activates a certain set of neurons. And there's a pattern to that. And that activation then has a certain output, which is like you feeling happy.

00:01:39:10 - 00:01:58:07
Nikhil Maddirala
So basically the idea is can we construct an artificial system that's somewhat similar to that with a bunch of interconnected neurons, and try to figure out patterns of activation that map to certain types of input? I it's very abstract, but I can actually go into an example, help.

00:01:58:07 - 00:02:16:02
Piyush Agarwal
Me understand it this way. Like I really like the way you like we the reason we went into machine learning. Yeah, is because you explained to me the challenge with the classical approach to programing. It's like when you do rule based programing, after a while you hit a wall, which is why you needed to find new novel approaches.

00:02:16:08 - 00:02:25:02
Piyush Agarwal
And the new novel approach was machine learning. Now, like, is it similar to that? Like, do we hit a wall with classical know that for which we need it?

00:02:25:05 - 00:02:26:00
Nikhil Maddirala
You hit is

00:02:26:00 - 00:02:35:03
Nikhil Maddirala
firstly you need to handcraft these features. Like you need to know what are the exact features I need like number of rooms or square footage or whatever.

00:02:35:03 - 00:02:44:28
Nikhil Maddirala
that's one, but I would say the main wall you hit is it can't work with unstructured data like this is. So there's a distinction between structured and unstructured data, right?

00:02:44:29 - 00:02:48:21
Nikhil Maddirala
Structured data is like this is the kind of thing you would put inside a SQL.

00:02:48:21 - 00:02:50:11
Piyush Agarwal
Database, like a spreadsheet or something, or a.

00:02:50:11 - 00:02:53:28
Nikhil Maddirala
Spreadsheet. You have rows and columns and this is very structured data.

00:02:54:02 - 00:02:55:12
Piyush Agarwal
Named road rules.

00:02:55:12 - 00:03:05:01
Nikhil Maddirala
And yeah, exactly what is unstructured data. Unstructured data is like a paragraph of text. Unstructured data is an image, right. That's completely unstructured. Right. So

00:03:05:01 - 00:03:16:07
Nikhil Maddirala
the classical ML is not good at dealing with unstructured data. I can't feed it an image I don't even know. Like what am I doing with this image? Like if you. So actually the one example I want to

00:03:16:07 - 00:03:19:10
Nikhil Maddirala
talk through for neural networks is,

00:03:19:10 - 00:03:20:11
Nikhil Maddirala
image recognition.

00:03:20:19 - 00:03:21:17
Nikhil Maddirala
So there's this,

00:03:21:17 - 00:03:34:07
Nikhil Maddirala
very famous like task. It's called handwriting digit recognition. There's a database called mNIST. I don't even know what it stands for, but basically it's a bunch of images of handwritten digits from 0 to 9.

00:03:34:07 - 00:03:44:01
Nikhil Maddirala
people handwrite like you, right? Seven, eight, whatever. And the goal, the task there is to figure out from the image what number is represented.

00:03:44:04 - 00:03:48:14
Nikhil Maddirala
Okay, okay. So think about that task. I have. I'm just giving you. It's like.

00:03:48:14 - 00:03:51:11
Piyush Agarwal
Dude, I don't even know where to start. I yeah.

00:03:51:13 - 00:03:52:00
Nikhil Maddirala
How would you.

00:03:52:00 - 00:03:52:15
Piyush Agarwal
Even. Yeah.

00:03:52:15 - 00:03:56:22
Nikhil Maddirala
What is the feature here. How do you extract a feature. So this stuff doesn't work. Yeah.

00:03:56:22 - 00:04:17:22
Piyush Agarwal
Because like I've talked about this before and I was like, I don't even know how to think about this. Like, first of all, how do you even get the machine to see that. Yeah. Handwritten thing like, I'm, I'm like lost. Yeah. Already. Yeah. Like, what is it? See it. Let me explain to you what I mean when you, let's say my my my baby girl Aisha, she's six months old, right?

00:04:17:22 - 00:04:18:10
Nikhil Maddirala
Yeah.

00:04:18:13 - 00:04:31:24
Piyush Agarwal
I'm going to show her an eight. Yeah. She does not know what it is for her. It's just an abstract squiggly line. But at least she can see that in a white piece of paper. There is a contrast

00:04:31:24 - 00:04:37:19
Piyush Agarwal
and she can determine that there's a squiggly line in the shape of an it. I'm saying how, how do you even show the machine.

00:04:37:20 - 00:04:38:28
Nikhil Maddirala
Yeah.

00:04:39:00 - 00:04:41:17
Piyush Agarwal
Because the machine only reads zeros and ones.

00:04:41:17 - 00:04:42:07
Nikhil Maddirala
Yeah.

00:04:42:10 - 00:04:52:21
Piyush Agarwal
First you have to tell the machine that this within the zeros and ones there is like a white paper in which there is a squiggly line in this pattern like that itself is seems so challenging.

00:04:52:22 - 00:05:02:05
Nikhil Maddirala
So that's actually a great question. I think that's a really good segue. You can't do that with this, but you can do it with neural networks. And I can explain to you exactly how we love you.

00:05:02:05 - 00:05:05:06
Piyush Agarwal
Amazing. Because I always do you understand my question like.

00:05:05:08 - 00:05:12:25
Nikhil Maddirala
I have the answer to your exact question. Okay. So this is abstractly what a neural network is.

00:05:12:25 - 00:05:22:25
Nikhil Maddirala
and this will become more clear with this example of the handwriting recognition that we're going to talk about. But there are some certain there's an input layer. And then these are the things that are called neurons.

00:05:22:25 - 00:05:25:06
Nikhil Maddirala
each neuron is like a neuron in your brain.

00:05:25:07 - 00:05:28:09
Nikhil Maddirala
We don't actually know if this is how the brain works at all. It's just,

00:05:28:09 - 00:05:30:11
Nikhil Maddirala
a theory, a hypothesis.

00:05:30:11 - 00:05:34:13
Nikhil Maddirala
but what happens is each neuron either gets activated or not.

00:05:34:13 - 00:05:41:17
Nikhil Maddirala
and then that signal passes through to the next layer. And finally there's an output layer that gives you the answer you want.

00:05:41:17 - 00:05:43:02
Nikhil Maddirala
this probably doesn't make sense.

00:05:43:02 - 00:05:45:01
Nikhil Maddirala
Now let's go through the example. And then

00:05:45:01 - 00:05:59:05
Nikhil Maddirala
we can see how it makes sense. Yeah. So yeah. So the input layer, it receives the inputs. The output layer is the final layer that creates like the response. And the hidden layer is what's doing the manipulations in between. Like it's doing the.

00:05:59:12 - 00:06:01:14
Piyush Agarwal
What what example. Every following here is.

00:06:01:14 - 00:06:02:08
Nikhil Maddirala
The handwritten.

00:06:02:08 - 00:06:07:10
Piyush Agarwal
Input. Often handwrite handwritten digit from anywhere between 0 and 9.

00:06:07:10 - 00:06:08:01
Nikhil Maddirala
029.

00:06:08:02 - 00:06:10:04
Piyush Agarwal
The goal of this thing this model.

00:06:10:04 - 00:06:12:15
Nikhil Maddirala
Is predict which digit that is. Yeah,

00:06:12:15 - 00:06:18:03
Nikhil Maddirala
exactly. So in that case actually let's think about what is the input layer to that. Okay.

00:06:18:03 - 00:06:21:16
Nikhil Maddirala
What am I going to input from the the image. Like

00:06:21:16 - 00:06:22:18
Nikhil Maddirala
do you have any ideas.

00:06:22:18 - 00:06:24:04
Piyush Agarwal
No I mean that's my question.

00:06:24:04 - 00:06:24:21
Nikhil Maddirala
Like okay.

00:06:24:22 - 00:06:39:10
Piyush Agarwal
Like how do you break down an image. Yeah. Do a machine a machine. Am I wrong in thinking that a machine can only see zeros and ones? Sure. Machine literally can only see a high voltage or low voltage, which is then represented in zero and one.

00:06:39:15 - 00:06:45:28
Nikhil Maddirala
So let's take a simplifying example. Let's say these images are all black and white okay. Or let's say they're grayscale.

00:06:45:28 - 00:06:47:27
Piyush Agarwal
That's like so yeah that's black and white. Yeah.

00:06:47:27 - 00:06:54:18
Nikhil Maddirala
So grayscale. And in this particular data set in mNIST they're 28 pixels by 28 pixels. Okay. And

00:06:54:18 - 00:07:01:12
Nikhil Maddirala
28 multiplied by 28 is 784. So there's 784 pixels.

00:07:01:12 - 00:07:01:23
Piyush Agarwal
Okay.

00:07:01:25 - 00:07:16:20
Nikhil Maddirala
Each pixel is an input, and the value of each pixel is between 0 to 1. One is if it's totally black, zero is if it's totally white. And in between is how great is that's what. That's how grayscale images are encoded.

00:07:16:23 - 00:07:21:10
Piyush Agarwal
Oh I see okay. So if it's an eight you will take like a square pixel size image.

00:07:21:10 - 00:07:24:29
Nikhil Maddirala
28 by 28 right. Yeah. You sequence it into 780.

00:07:24:29 - 00:07:30:27
Piyush Agarwal
For all the places where it's actually eight will let's say it'll be one and all the places it's just white paper is.

00:07:30:27 - 00:07:42:09
Nikhil Maddirala
Zero. Yeah. Or that's if it's black and white. That's what it is. If it's grayscale, it's more sophisticated. You have an entire scale between 0 and 1. How one is most black and zero is most white.

00:07:42:09 - 00:07:46:22
Piyush Agarwal
For simplicity, let's take black and white is like, yeah, okay. But it's like, yeah.

00:07:46:28 - 00:07:48:18
Nikhil Maddirala
Then it's zeros and ones. Exactly.

00:07:48:19 - 00:07:52:11
Piyush Agarwal
Pen to paper is one. Yeah. Only paper is zero. Let's simplify it okay?

00:07:52:13 - 00:07:53:24
Nikhil Maddirala
Okay. Yeah. Okay.

00:07:53:24 - 00:07:55:22
Nikhil Maddirala
yeah. That's a simplified example.

00:07:55:22 - 00:08:04:08
Nikhil Maddirala
realistically it would be grayscale because there are some places where it's like maybe. Yeah, yeah, yeah. And it's easy to generalize this to a color image, like,

00:08:04:08 - 00:08:10:28
Nikhil Maddirala
because color you just, you break it down into RGB values and you say 70, 84 pixels times three for each pixel.

00:08:10:28 - 00:08:14:21
Nikhil Maddirala
I'll give you an R, G and B value. Right. That's how you convert it.

00:08:14:21 - 00:08:22:03
Piyush Agarwal
Yeah. Oh that's another computer cool computer thing that you can break down. I mean well yeah computers adopted it from the primary colors. But yeah, I see what you mean. Yeah.

00:08:22:03 - 00:08:23:03
Nikhil Maddirala
You can, you can create.

00:08:23:03 - 00:08:25:01
Piyush Agarwal
Any color from RGB. Exactly. Yeah.

00:08:25:05 - 00:08:48:06
Nikhil Maddirala
So in this case let's just simplify it and say it's only grayscale. Or you can think about it's only black and white 7084 pixels as the input layer. What you want out of the output layer is finally a number right 129. But realistically what happens is your output layer has nine neurons. Each of them are numbered 0 to 9, and the output is a prediction of between,

00:08:48:06 - 00:08:52:16
Nikhil Maddirala
probability of how likely is it that this image is an eight.

00:08:52:18 - 00:08:54:15
Piyush Agarwal
This is supposed to make it a seven.

00:08:54:20 - 00:08:54:27
Nikhil Maddirala
Yeah.

00:08:55:04 - 00:08:57:04
Piyush Agarwal
And it is. My handwriting is.

00:08:57:06 - 00:09:02:15
Nikhil Maddirala
Yeah. Because ideally like, you know, sometimes there will be and it won't always be right. Percent.

00:09:02:17 - 00:09:04:25
Piyush Agarwal
So like a 4th May look like a nine on how.

00:09:04:25 - 00:09:05:18
Nikhil Maddirala
You do it. Yeah.

00:09:05:19 - 00:09:06:04
Piyush Agarwal
Got it. Okay.

00:09:06:04 - 00:09:23:19
Nikhil Maddirala
So the goal is that when you feed in a seven all the pixels go in. Right. And the one, the layer corresponding to seven is the one that lights up the most. Right? Okay. So let me show you an example of what this. Oh let's call this neural network. So the input layer is, as I said 784.

00:09:23:19 - 00:09:25:06
Nikhil Maddirala
input neurons okay.

00:09:25:06 - 00:09:27:05
Nikhil Maddirala
That's how big the input layer is.

00:09:27:05 - 00:09:37:14
Nikhil Maddirala
because we said 28 by 28 and that's each pixel. And yeah, you can think of these greens as the activations. The blank ones are not activated in this case. Actually, they're shades of green,

00:09:37:14 - 00:09:46:07
Nikhil Maddirala
because it's grayscale. And then this output layer for this seven, it's activating the seven, which means that it correctly recognized that it is seven.

00:09:46:09 - 00:09:55:05
Nikhil Maddirala
And then there are these two things in between called hidden layers. That's where the manipulation is happening. That's where the weights and parameters are coming.

00:09:55:05 - 00:09:56:00
Piyush Agarwal
That's the magic. That's it.

00:09:56:01 - 00:10:17:16
Nikhil Maddirala
That's right. Yeah. Exactly. That's where the magic is happening. So it's just like the case earlier we talked about with linear regression. First you start with some random way of activating things and then you evaluate the loss. And then you look at how do I optimize this. So actually let's go back to this list here and think about what all these things here correspond to.

00:10:17:18 - 00:10:25:02
Nikhil Maddirala
maybe you can start with features. That's an easy one to start with. In this case features are there 784 features each pixel value.

00:10:25:02 - 00:10:26:00
Piyush Agarwal
That's it. Right.

00:10:26:02 - 00:10:28:14
Nikhil Maddirala
We're in the home prices case. Remember we had square.

00:10:28:18 - 00:10:29:01
Piyush Agarwal
Footage.

00:10:29:03 - 00:10:39:06
Nikhil Maddirala
Or. Yeah we had only one feature. In this case we have 784 features that we're putting in model architecture. It's a little complex, but it's what I showed you in that diagram.

00:10:39:06 - 00:10:41:17
Piyush Agarwal
Instead of linear regression it's a neural network.

00:10:41:17 - 00:10:44:25
Nikhil Maddirala
It's a neural network okay. Yeah. Parameters. Okay.

00:10:44:25 - 00:10:48:08
Nikhil Maddirala
before we talk about parameters, let's talk about the objective function.

00:10:48:08 - 00:10:55:26
Nikhil Maddirala
the objective function again as always, is to minimize the loss. So just as remember in the regression case we.

00:10:55:26 - 00:10:58:14
Piyush Agarwal
Talked about start with one random line and then you move it towards.

00:10:58:14 - 00:10:59:22
Nikhil Maddirala
Yeah. But what is the

00:10:59:22 - 00:11:07:14
Nikhil Maddirala
objective function we're measuring. What is the distance between that line and the actual data block. Yeah. So in this case what we do is or

00:11:07:14 - 00:11:08:14
Nikhil Maddirala
once we,

00:11:08:14 - 00:11:17:06
Nikhil Maddirala
create a model, we put all our data through it and we measure the distance between the predictions and the actual values. That's always what a,

00:11:17:06 - 00:11:18:05
Nikhil Maddirala
objective function is.

00:11:18:05 - 00:11:38:14
Nikhil Maddirala
You want to minimize the distance between your predicted values and the actual values. Right. So the predicted values, you know, for the data set, you fed it, it may be like zero, one, two, whatever actual values may be something completely different. And you come up with a way to measure the distance between those two. That's the objective function that you're trying to minimize, right.

00:11:38:16 - 00:11:54:01
Piyush Agarwal
And then I see what do you mean. So you're saying that in this case in this model you'll start with let's say 100 examples. Yeah. Of like that's the training data. Yeah. The training data would be a bunch of places, a papers 28 by 20 pixels, where you written a bunch of numbers

00:11:54:01 - 00:11:55:08
Piyush Agarwal
and obviously, you know what the number is.

00:11:55:08 - 00:12:01:27
Piyush Agarwal
So you're training the data. So you will start with your 784 input data. You obviously know what the answer is.

00:12:01:27 - 00:12:09:07
Piyush Agarwal
So probability will be one. Right. But what's happening in like like what actually happens during the training.

00:12:09:07 - 00:12:17:09
Nikhil Maddirala
Let's talk about that. So let's talk about what's happening in these hidden layers. So there's a thing called an activation function here which is

00:12:17:09 - 00:12:30:07
Nikhil Maddirala
there's a there are lines connecting every node to every other node in the next layer. In this architecture we're just assuming I have two hidden layers. Each of them has 16 neurons. In this example, is this arbitrary?

00:12:30:14 - 00:12:53:07
Nikhil Maddirala
We can't really get into like why there are 16, right. arbitrarily we assume there are two layers and 16. This is describing what's happening at the first layer for each connection between a neuron in this layer and a neuron in this layer. It has a weight. Right. Okay. So these all have activations that are A1 through a n and then they all have weights.

00:12:53:07 - 00:13:01:06
Nikhil Maddirala
So suppose here I want to compute the activation here. Why is this green. Why is that lighting up as opposed to not lighting up. So what I'm doing is

00:13:01:06 - 00:13:15:04
Nikhil Maddirala
I compute the activation function of that neuron. This neuron is connected to 784 input neurons. And through this function here, it's just a random set of weights and biases. I determine whether they should be activated or not.

00:13:15:06 - 00:13:21:04
Nikhil Maddirala
And it's going to give me like a one or a zero, or it could be anything between a zero and one.

00:13:21:04 - 00:13:31:04
Nikhil Maddirala
so that's it's going to output an activation. And then the same thing is happening at the next layer again, but much smaller because I have only 16 now. And the same thing is happening at the final layer.

00:13:31:06 - 00:13:33:04
Nikhil Maddirala
So there are only 16 things here.

00:13:33:06 - 00:13:37:26
Piyush Agarwal
So does it progressively reduce the hidden layer. So like the first 16, the ones I.

00:13:37:26 - 00:13:41:18
Nikhil Maddirala
In this case they're both 16. But it typically does reduce.

00:13:41:18 - 00:13:45:03
Piyush Agarwal
Yeah it reduces. yeah. And then eventually you'll come to some.

00:13:45:05 - 00:14:02:12
Nikhil Maddirala
Idea of what we think is going on here. We really don't know is that each layer is developing some abstraction over the like the first layer literally looking at individual pixels. The next layer we think is learning some patterns behind those pixels. Like, oh, does it have a straight line?

00:14:02:14 - 00:14:03:11
Piyush Agarwal
does it have a loop?

00:14:03:16 - 00:14:12:23
Nikhil Maddirala
Something like this. And then each subsequent layer is a higher level of abstraction until you get to the final layer that tells you what number it actually is, right?

00:14:12:23 - 00:14:13:12
Piyush Agarwal
Right. Okay.

00:14:13:12 - 00:14:18:09
Nikhil Maddirala
That's what's happening in this. So now I just want us to think about how many,

00:14:18:09 - 00:14:34:05
Nikhil Maddirala
parameters there are here. So each neuron is connected to each other neuron in the next layer 784 connected to 16 neurons here. And each one also has a bias. Bias just means like in this case, like this

00:14:34:05 - 00:14:37:22
Nikhil Maddirala
parameter. That's not connect. That's not a coefficient of any feature.

00:14:37:22 - 00:14:38:27
Nikhil Maddirala
We just call that the bias.

00:14:38:28 - 00:14:40:27
Piyush Agarwal
Right? Right. 100 in this case is about one.

00:14:40:27 - 00:14:46:13
Nikhil Maddirala
Hundred in this case. So in this case like each neuron has its own bias. And n weights.

00:14:46:13 - 00:14:59:18
Piyush Agarwal
Are you saying that. So so the hidden layers right now for this for simplicity's sake there are only two hidden layers. But it could be any number of hidden layers. You're saying the first one, when you start training the data, you just start with something random?

00:14:59:22 - 00:15:24:18
Nikhil Maddirala
Yes. It's not the okay, first let's think about what all the parameters are. Let's go here and see there are going to be 784 times 16. This is the number of parameters for these connections, right? 16 times 16 is the number of parameters for these connections. And 16 times ten is the number of parameters there. And there are this many biases because each neuron has its own bias.

00:15:24:20 - 00:15:30:27
Nikhil Maddirala
So there are 13,000 parameters in sorry, I'm getting something in my eye.

00:15:30:27 - 00:15:49:20
Nikhil Maddirala
there are 13,000 parameters in this model. So remember we talked about with linear regression. All that's happening is first you initialize a random set of parameters and then you move towards the right direction to get the right parameters that are minimizing your loss function.

00:15:49:20 - 00:15:52:01
Nikhil Maddirala
loss function is just the inverse of objective function.

00:15:52:01 - 00:15:55:03
Nikhil Maddirala
Here that are optimizing your objective function.

00:15:55:03 - 00:16:03:06
Nikhil Maddirala
and the way this happens, it's a combination of something called backpropagation and gradient descent, which we talked about earlier. So when,

00:16:03:06 - 00:16:22:08
Nikhil Maddirala
there's an error here, like, you know, suppose it recognize the ones as zeros instead, based on the error that we find from the objective function value, it knows like how to propagate backwards and adjust the weights to get to the right direction to optimize the objective function.

00:16:22:08 - 00:16:36:27
Nikhil Maddirala
So that's all that's happening is it's it's just like linear regression on like a hyper dimensional scale. You're just tweaking the parameters, right? Like in an intelligent nonrandom way. You could theoretically be like, you remember originally you said, like,

00:16:36:27 - 00:16:43:20
Nikhil Maddirala
we can draw a thousand lines for regression and find which is the best. Theoretically, you could do this if you had infinite computational power.

00:16:43:20 - 00:16:54:05
Nikhil Maddirala
Right? You could just generate, like infinite combinations of these 13,000 parameters and just say, select the best one. That itself would be like a way to do it. Right.

00:16:54:05 - 00:17:16:22
Nikhil Maddirala
but since we don't have infinite computational power, we want to optimize. We want to test only the relevant combinations. So every time we know which direction to go in based on this gradient descent thing, and it moves in that direction and only tweaks the parameters in one way until it gets to the optimal set of parameters that can most accurately predict the disease.

00:17:16:25 - 00:17:33:02
Piyush Agarwal
So what I understand from this is the the breakthrough over here with neural nets. Yeah. So like I'm going to like summarize our story. Our story is classical programing hits a wall. Then you have classical ML. Yeah. Hits a wall. Yeah. Then the breakthrough is neural nets.

00:17:33:02 - 00:17:34:03
Nikhil Maddirala
Yeah.

00:17:34:05 - 00:17:40:13
Piyush Agarwal
What's the breakthrough with classical ML. The breakthrough is instead of you programing the rules, the algorithm figures out.

00:17:40:20 - 00:17:41:11
Nikhil Maddirala
Yes.

00:17:41:13 - 00:17:46:10
Piyush Agarwal
What is the breakthrough here? Is it the hidden layers and this back propagation thing?

00:17:46:10 - 00:18:02:20
Nikhil Maddirala
The breakthrough is that you don't need to figure out how to structure the data. Like remember, in the classical ML, I have to structure the data and say this is the feature. Like the feature is like the number of rooms or the number of houses right here. You don't have to structure the data. I just give it 700 and.

00:18:02:20 - 00:18:05:16
Piyush Agarwal
80 oh oh. I see what you mean. Yeah. You're just giving it raw pixels.

00:18:05:16 - 00:18:10:11
Nikhil Maddirala
Yeah, exactly. Or in the case of language models which we'll talk about next is raw text.

00:18:10:18 - 00:18:12:03
Piyush Agarwal
Right. You will give it. Right. Okay.

00:18:12:04 - 00:18:25:15
Nikhil Maddirala
I don't have to struggle to the data at all okay. And I don't have to structure the features like because again in classical ML typically there's a lot of handcrafting of features that you have to do in a regression model.

00:18:25:15 - 00:18:35:28
Nikhil Maddirala
sometimes you will find that like linear regression may just be the wrong example. Linear regression means there's a straight line relationship between the two variables you're trying to plot, but maybe there isn't.

00:18:35:28 - 00:18:50:24
Nikhil Maddirala
Maybe it's a polynomial like equation. So it takes a lot of time to customize and find the right one that fits your data. In this case, you have to do none of that right? It's just an abstract structure that can apply to, like many different types of data sets.

00:18:50:26 - 00:19:09:09
Piyush Agarwal
It also sounds to me like every next evolution, we're making it simpler in some sense. Obviously, we're taking advantage of the massive computational power, but can you? So like let me explain how I mean, so in the first case, we had to do the one on one. Like we had to create the rules, right?

00:19:09:09 - 00:19:10:00
Nikhil Maddirala
Yeah.

00:19:10:03 - 00:19:12:19
Piyush Agarwal
Now in classical ML you don't have to create the rule.

00:19:12:25 - 00:19:14:02
Nikhil Maddirala
We get the hazy. Yeah.

00:19:14:04 - 00:19:21:29
Piyush Agarwal
In the classical level you don't have to get the yeah the rules. So like from the programmer's point of view it's simplifying your job. Right. Like you don't have to do the rules,

00:19:21:29 - 00:19:24:02
Piyush Agarwal
but you still have to like figure out the features and work.

00:19:24:03 - 00:19:28:01
Nikhil Maddirala
Yeah, but there are some trade offs that come here because,

00:19:28:01 - 00:19:45:09
Nikhil Maddirala
one thing that goes down is interpretability. Like, I don't like in this model. Now, if you tell me that look, based on this input, I got this output and you tell me like, why? Yeah. I can't give you the answer. I'll say there are some 13,000 parameters in there.

00:19:45:11 - 00:19:48:03
Nikhil Maddirala
I don't know what they're doing. They gave me this answer.

00:19:48:06 - 00:19:53:21
Piyush Agarwal
That's another thing. You have answered one of my questions, because a lot of times I've heard people say,

00:19:53:21 - 00:19:57:03
Piyush Agarwal
like the black box nature of things. Yeah. You don't really understand why this.

00:19:57:03 - 00:20:04:00
Nikhil Maddirala
Is where the black box comes in. Because in the case of the linear regression, it was very easy. If you give me,

00:20:04:00 - 00:20:22:29
Nikhil Maddirala
I know exactly what parameter corresponds to which feature, because the parameter is the coefficient of a feature. And if you give me a new house and my model predicts that your new house has a low value, and you ask me, hey, why does my house have a low value, I can tell you exactly why it has a low value, right?

00:20:23:00 - 00:20:30:22
Nikhil Maddirala
I'll say, hey, my model multiplies your number of rooms by 50, right? Your number of rooms is low. So like your house price which is low.

00:20:30:23 - 00:20:32:10
Piyush Agarwal
That's a really good example. That's why

00:20:32:10 - 00:20:40:16
Piyush Agarwal
like, if he like one of the concerns that people have is like increasingly with algorithms making a lot of our decision which which is happening. Right. Like increasingly,

00:20:40:16 - 00:20:47:03
Piyush Agarwal
it's going to make a decision which might seem unfair to us, and there is nothing we can do about it. And no one will be able to explain why that happened.

00:20:47:06 - 00:20:56:27
Piyush Agarwal
Is that why we're like, why not? One of the examples I can think of is like, imagine you apply for something and if your applications deny an abstract, I'm generalizing it like some application and you just denied.

00:20:56:28 - 00:20:58:25
Nikhil Maddirala
Credit card application. This is a good one.

00:20:58:25 - 00:21:04:07
Piyush Agarwal
And imagine like your application is denied, but you were not given an explanation on why that happens.

00:21:04:09 - 00:21:26:23
Nikhil Maddirala
Because no one knows the explanation. Like once you get into these, once you increase the like two parameters, ten parameters. Yes, it's okay, I can explain to you what's happening. Once you get to 13,000 parameters, it's like not no one understands what these parameters are doing. Right. And there's some effort going on right now into ML explainability. But it's hard.

00:21:26:23 - 00:21:33:13
Nikhil Maddirala
There's a trade off between models that are more powerful versus models that are more explainable.

00:21:33:16 - 00:21:47:02
Piyush Agarwal
And that's just right I think. Yeah that's it okay I understand. So with every new breakthrough you're getting more powerful models. But obviously there's a trade off here and the trade off is you losing. Exactly you into what's actually happening.

00:21:47:05 - 00:21:52:19
Nikhil Maddirala
Because like this thing here is a complete black box. Like nobody, no human being can look at this and understand, like.

00:21:52:19 - 00:21:54:02
Piyush Agarwal
Yeah, it's a scary picture.

00:21:54:03 - 00:22:05:28
Nikhil Maddirala
Yeah. Like, you know, a mathematically maybe you can understand some of the equations, but literally you're just saying like there's a bunch of random. Usually this is treated as a vector in,

00:22:05:28 - 00:22:14:15
Nikhil Maddirala
because you don't say these are 70, 84 individual values. You treat it as one vector with 70, 84 dimensions or one. Yeah. And,

00:22:14:15 - 00:22:18:04
Nikhil Maddirala
it's all these, these multiplications that are happening.

00:22:18:06 - 00:22:27:08
Nikhil Maddirala
Usually they're like matrix multiplications because you would treat them all together. So and that's actually a reason why GPUs are optimized for this task.

00:22:27:08 - 00:22:52:07
Piyush Agarwal
Actually doesn't understand as well. Like what Nvidia if you have like I don't know if you follow stock prices or not, but like if one of the interesting stocks that you could followed over the last five, ten years is the Nvidia stock, it's insane how much it's increasing. I think one of the analogies that people give is like they've observed that whenever a gold rush is happening, the people that tend to make a lot of money are the people who supply coal miners with their equipment.

00:22:52:09 - 00:22:52:16
Piyush Agarwal
Yeah.

00:22:52:17 - 00:22:53:18
Nikhil Maddirala
And picks and shovels.

00:22:53:19 - 00:23:08:19
Piyush Agarwal
Yeah. And in this case, like if you extend this analogy to like, I like if there's a gold rush happening. Yeah. Then Nvidia is getting an edge. I understand all that. Like I don't fully understand like what is it. It's used to make graphics cards. What happens suddenly.

00:23:08:19 - 00:23:31:11
Nikhil Maddirala
So why do you think cards are actually a type of processor that's optimized for a very narrow function, whereas CPU, GPU versus CPU, CPU is a general purpose computing right machine. GPU is optimized for a very specific function like of rendering graphics, which actually turns out is ultimately matrix algebra. And,

00:23:31:11 - 00:23:38:19
Nikhil Maddirala
training of neural networks is all matrix algebra, because at the end of the day, you're doing like millions of matrix multiplications.

00:23:38:19 - 00:23:43:16
Nikhil Maddirala
That's what's going on here. Every time you compute an activation function, every time you have to,

00:23:43:16 - 00:24:04:22
Nikhil Maddirala
do that for each and every layer, like, you know, there's 13,000 parameters. So it's just a bunch of matrix multiplication. And right, a CPU can do that, but it's not optimized for that. So whereas if you take a narrow machine like a GPU that's just optimized for doing one type of operation, it can scale, do it more cheaply, do it more efficiently.

00:24:04:26 - 00:24:05:27
Piyush Agarwal
Okay. That makes sense. Okay.

00:24:05:28 - 00:24:21:01
Nikhil Maddirala
With the rise of neural networks that made these matrix multiplications, like really important. And that's like most of your compute resources are spent doing matrix multiplication. It made sense that we're going to build specialized hardware GPUs and then Google

00:24:21:01 - 00:24:22:01
Nikhil Maddirala
built TPUs.

00:24:22:01 - 00:24:22:27
Piyush Agarwal
Yeah. Tensor processors.

00:24:22:27 - 00:24:30:20
Nikhil Maddirala
Yes. Those are also optimized for matrix multiplication. And I think they're actually even further optimized if you use Google's,

00:24:30:20 - 00:24:34:11
Nikhil Maddirala
TensorFlow library for doing that.

00:24:34:13 - 00:24:41:10
Piyush Agarwal
yeah, I want to get into that. Not now whenever you think it's appropriate. But I don't understand like what is TensorFlow? What. Yeah.

00:24:41:11 - 00:24:43:18
Nikhil Maddirala
Oh yeah we can talk about that. It TensorFlow is just

00:24:43:18 - 00:24:56:08
Nikhil Maddirala
a Python library. TensorFlow and PyTorch are the two common libraries that make it easy to do. Exactly. Like, imagine if you want to write a program that creates a neural network. Yeah, like you don't want to have to like sit in.

00:24:56:09 - 00:25:04:29
Piyush Agarwal
I only bother about this and this. Like I don't want to be bothered with this. So TensorFlow does all the hidden layer PyTorch. Yes they do the hidden layers.

00:25:04:29 - 00:25:14:04
Nikhil Maddirala
They do everything. They create all that. You can define how many layers you want. It is an abstraction layer on top of this neural network. Like. And it does the

00:25:14:04 - 00:25:23:16
Nikhil Maddirala
the training algorithm. Like you don't have to code how to do gradient descent, how to do backpropagation. They have that built in. So that's ultimately they're just like high level,

00:25:23:16 - 00:25:27:29
Nikhil Maddirala
Python libraries that make it easy for you to interact with these.

00:25:28:01 - 00:25:28:06
Piyush Agarwal
Low.

00:25:28:06 - 00:25:34:19
Nikhil Maddirala
Level content. Makes sense. Yeah, okay. And yeah, Facebook built PyTorch, Google build TensorFlow.

00:25:34:19 - 00:25:41:15
Nikhil Maddirala
they're both pretty popular. I don't know how things will change now with like LMS and stuff. We have to see what happens.

00:25:41:15 - 00:25:42:04
Nikhil Maddirala
yeah. But yeah.

00:25:42:04 - 00:25:47:19
Piyush Agarwal
So that's. Yeah. So yeah. So the big breakthrough, the first with the classical, I'm sorry if

00:25:47:19 - 00:25:59:04
Piyush Agarwal
I repeat too much is just not worth signing. Yeah. With classical ML is you don't have to write rules. With neural nets is you don't even have to give the features or structure of the data.

00:25:59:04 - 00:25:59:22
Nikhil Maddirala
Exactly.

00:25:59:22 - 00:26:02:16
Piyush Agarwal
So we're giving well and that way is becoming more powerful.

00:26:02:16 - 00:26:03:05
Nikhil Maddirala
It's automated.

00:26:03:05 - 00:26:12:12
Piyush Agarwal
And it's also like in some sense like less work even. It sounds like hey you don't have to read us. Hey, you don't have to give a feature. Just yeah, just give it all in all pixels and I'll do all that for you.

00:26:12:12 - 00:26:12:21
Nikhil Maddirala
Yeah.

00:26:12:25 - 00:26:20:19
Piyush Agarwal
All of you. See, it requires a lot more computational power as you as can be seen with scary looking, mad, scary looking graphs.

00:26:20:19 - 00:26:21:02
Nikhil Maddirala
Yeah.

00:26:21:03 - 00:26:22:28
Piyush Agarwal
And you losing explainability.

00:26:22:28 - 00:26:24:03
Nikhil Maddirala
Yeah. Exactly.

00:26:24:05 - 00:26:28:03
Piyush Agarwal
So yes. So I understand. So neural nets can do stuff like,

00:26:28:05 - 00:26:31:14
Nikhil Maddirala
This prediction if you hear the deep learning,

00:26:31:14 - 00:26:39:07
Nikhil Maddirala
term, it just refers to a neural network where you have more than one hidden layer. That's how deep learning means. Oh, it means the network is deep.

00:26:39:08 - 00:26:52:11
Piyush Agarwal
So deep learning is a type of neural network. Yes, exactly. So if you go back to the circle, you there's AI, there's classical animal. And then within ML there's neural nets. And deep learning is type of that where it's more than one hidden layer. Yeah.

00:26:52:15 - 00:26:53:09
Nikhil Maddirala
Exactly. Okay.

00:26:53:10 - 00:27:01:21
Piyush Agarwal
Yes okay. So now we're down to deep learning which is like more hidden layers. And now we can do more complex things like image recognition. Yeah. This is just a basic example like.

00:27:01:21 - 00:27:06:08
Nikhil Maddirala
And you can also do text right. Yeah. So that's the next thing.

00:27:06:09 - 00:27:09:09
Nikhil Maddirala
So we wanted to talk about generative AI right.

00:27:09:11 - 00:27:09:20
Piyush Agarwal
Yeah.

